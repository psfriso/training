{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# pySpark Basics\n",
    "### In Pictures and By Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark\n",
    "<img src=\"img/spark-cluster-overview.png\"/>\n",
    "[ from https://spark.apache.org/docs/2.2.0/cluster-overview.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.88.127:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=test>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc.stop() # uncomment in case you want to rerun it\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"test\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# this will be the size of our sample dataset\n",
    "SIZE = 10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resilient Distributed Dataset (RDD)\n",
    "It is lazy - creating a  doesn't actually execute the computation.\n",
    "<img src=\"img/rdd-create.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 3.03 ms, total: 4.94 ms\n",
      "Wall time: 257 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# that is lazy and quick\n",
    "%time rdd = sc.parallelize(range(1, SIZE), 1).map(lambda x: math.sqrt(x))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDD executed\n",
    "You need to run a Spark 'action' (eg count, reduce, collect, first, take, save..., foreach) for the computation to actually execute.\n",
    "<img src=\"img/count.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 5.36 ms, total: 16.3 ms\n",
      "Wall time: 3.03 s\n",
      "CPU times: user 4.87 ms, sys: 2.2 ms, total: 7.07 ms\n",
      "Wall time: 2.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd\n",
    "# The second run time is similar to the first one:\n",
    "%time rdd.count()\n",
    "%time rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDD cached\n",
    "Cached RDD will be reused at subsequent runs.\n",
    "<img src=\"img/count-cached.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 ms, sys: 1.37 ms, total: 2.7 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rddcache = rdd.cache()\n",
    "rddcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.59 ms, sys: 3.83 ms, total: 9.42 ms\n",
      "Wall time: 3.14 s\n",
      "CPU times: user 5.33 ms, sys: 2.82 ms, total: 8.14 ms\n",
      "Wall time: 799 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9999999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice how the second run is quicker now below:\n",
    "%time rddcache.count()\n",
    "%time rddcache.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q: why is rdd suddenly quicker? (hint: check the UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.95 ms, sys: 4.29 ms, total: 11.2 ms\n",
      "Wall time: 783 ms\n",
      "PythonRDD[1] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[1] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "%time rdd.count()\n",
    "print(rdd)\n",
    "print(rddcache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation Distribution\n",
    "Using more partitions yields quicker execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single Partition\n",
    "<img src=\"img/single-part.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 ms, sys: 1.22 ms, total: 5.32 ms\n",
      "Wall time: 2.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9999999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,SIZE), numSlices=1).cache()\n",
    "rdd.count()\n",
    "%time rdd.map(lambda x: math.sqrt(x)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiple partitions\n",
    "<img src=\"img/multi-part.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.01 ms, sys: 1.61 ms, total: 7.61 ms\n",
      "Wall time: 788 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9999999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,SIZE), numSlices=8).cache()\n",
    "rdd.count()\n",
    "%time rdd.map(lambda x: math.sqrt(x)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reduce vs GroupBy\n",
    "Sample problem: group items and return sums of elements in each group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/group.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 ms, sys: 4.44 ms, total: 21.7 ms\n",
      "Wall time: 4.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 49999990000000),\n",
       " (1, 49999995000000),\n",
       " (2, 50000000000000),\n",
       " (3, 50000005000000)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,SIZE*2), numSlices=8).map(lambda x: (x%4, x)).cache()\n",
    "rdd.count()\n",
    "%time rdd.groupByKey().map(lambda t: (t[0], sum(t[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/reduce.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 3.59 ms, total: 14.5 ms\n",
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 49999990000000),\n",
       " (1, 49999995000000),\n",
       " (2, 50000000000000),\n",
       " (3, 50000005000000)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,SIZE*2), numSlices=8).map(lambda x: (x%4, x)).cache()\n",
    "rdd.count()\n",
    "%time rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More In Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ui-jobs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ui-job.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ui-stage.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ui-storage.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ui-storage-rdd.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ui-executors.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioners, shuffling etc\n",
    "Be careful when using operations like joins, repartition, coalesce, ...ByKey as they might have to shuffle the data to, for example, put all the items with the same key on one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 ms, sys: 2.1 ms, total: 6.91 ms\n",
      "Wall time: 1.67 s\n",
      "None\n",
      "CPU times: user 13.9 ms, sys: 5.49 ms, total: 19.4 ms\n",
      "Wall time: 2.33 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 6249995000000),\n",
       " (1, 6249996250000),\n",
       " (2, 6249997500000),\n",
       " (3, 6249998750000),\n",
       " (4, 6250000000000),\n",
       " (5, 6250001250000),\n",
       " (6, 6250002500000),\n",
       " (7, 6250003750000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys are evenly spread out on partitions\n",
    "rdd = sc.parallelize(range(1,SIZE), numSlices=8).map(lambda x: (x%8, x)).cache()\n",
    "%time rdd.count()\n",
    "print(rdd.partitioner)\n",
    "%time rdd.groupByKey().mapValues(lambda x: sum(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![1](img/Shuffling-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.44 ms, sys: 2.14 ms, total: 7.58 ms\n",
      "Wall time: 8.58 s\n",
      "<pyspark.rdd.Partitioner object at 0x105e1d080>\n",
      "CPU times: user 3.02 ms, sys: 1.93 ms, total: 4.95 ms\n",
      "Wall time: 6.43 ms\n",
      "<pyspark.rdd.Partitioner object at 0x105e1d080>\n",
      "CPU times: user 6.46 ms, sys: 2.86 ms, total: 9.32 ms\n",
      "Wall time: 1.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 6249995000000),\n",
       " (1, 6249996250000),\n",
       " (2, 6249997500000),\n",
       " (3, 6249998750000),\n",
       " (4, 6250000000000),\n",
       " (5, 6250001250000),\n",
       " (6, 6250002500000),\n",
       " (7, 6250003750000)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same partitioner is used\n",
    "rdd = sc.parallelize(range(1,SIZE), numSlices=8).map(lambda x: (x%8, x)).partitionBy(numPartitions=8).cache()\n",
    "%time rdd.count()\n",
    "print(rdd.partitioner)\n",
    "%time rdd1=rdd.groupByKey().mapValues(lambda x: sum(x))\n",
    "print(rdd1.partitioner)\n",
    "%time rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![3](img/Shuffling-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.43 ms, sys: 2.1 ms, total: 7.53 ms\n",
      "Wall time: 5.21 s\n",
      "<pyspark.rdd.Partitioner object at 0x105e250f0>\n",
      "CPU times: user 4.11 ms, sys: 1.62 ms, total: 5.73 ms\n",
      "Wall time: 11.1 ms\n",
      "CPU times: user 6.05 ms, sys: 2.8 ms, total: 8.86 ms\n",
      "Wall time: 6.13 s\n",
      "<pyspark.rdd.Partitioner object at 0x105dffda0>\n"
     ]
    }
   ],
   "source": [
    "# partitioner is changed in the middle - causing double shuffle\n",
    "rdd = sc.parallelize(range(1,SIZE), numSlices=8).map(lambda x: (x%8, x)).partitionBy(8, lambda x: 1).cache()\n",
    "%time rdd.count()\n",
    "print(rdd.partitioner)\n",
    "%time rdd1=rdd.groupByKey().mapValues(lambda x: sum(x))\n",
    "%time rdd1.collect()\n",
    "print(rdd1.partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![2](img/Shuffling-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## pySpark\n",
    "<img src=\"img/pyspark.png\" />\n",
    "[ from https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HADOOP YARN\n",
    "<img src=\"img/yarnflow1.png\" />\n",
    "[ from https://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HADOOP YARN\n",
    "<img src=\"img/debt.jpg\" />\n",
    "[ from https://www.kdnuggets.com/2017/10/data-science-systems-engineering-approach.html ]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
