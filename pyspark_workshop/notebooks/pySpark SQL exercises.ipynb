{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create SparkSession object - entry point for all Spark computations. We're also loading some data and saving them as views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark = env_setup.getSession(local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign each part of a query to a variable. __table()__ method will return a Dataframe which contains a structured dataset. To see what's inside it we can print its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")\n",
    "\n",
    "print(\"# schema of both tables\")\n",
    "sales_df.printSchema()\n",
    "item_prices_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print first rows with __show()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# sample results from both tables\")\n",
    "sales_df.show()\n",
    "item_prices_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe transformations are evaluated lazily. If we chain multiple operations together they will be invoked only when we call an action. To see what's the current *execution plan* we can call __explain()__ method. It will work even for a simple selection from a view like here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# query execution plan for this simple select\")\n",
    "sales_df.explain()\n",
    "item_prices_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It prints a one-step operation, which is a csv FileScan, which is correct, because our table was read from such file. We can also see the schema along with some other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SQL support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL's name is rather intuitive - we can use Spark to execute SQL queries on our Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = spark.sql('select item_id, transaction_date from sales where shop_id = \"SHOP_1\" order by transaction_date desc')\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex1. Use plain SQL query select all transactions with quantity between 2 and 4 (inclusive). Show all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex2. Print mean unit price for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataframe operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysts usinq plain SQL API may be enough, but its much better to use method invocation and chaining to transform out Dataframes. Almost every operation in SQL can be translated to some methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = sales_df.select(\"item_id\", \"transaction_date\")\\\n",
    "    .filter(f.col(\"shop_id\") == \"SHOP_1\")\\\n",
    "    .orderBy(f.col(\"transaction_date\").desc())\n",
    "string_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can use strings to specify column names, but there is another, more dynamic, option to treat columns as fields in a dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_df = sales_df.select(sales_df.item_id, sales_df.transaction_date)\\\n",
    "    .filter(sales_df.shop_id == \"SHOP_1\")\\\n",
    "    .orderBy(sales_df.transaction_date.desc())\n",
    "field_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same and actually all these queries will be executed in __exactly__ the same way by Spark. The only difference is the query translation step. Afterwards, when spark has an execution plan of a query it will treat it in the same way regardless of used API. To verify that let's see physical plans of all three queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plain SQL API execution plan\")\n",
    "sql_df.explain()\n",
    "print(\"\\n Columns as strings approach\")\n",
    "string_df.explain()\n",
    "print(\"\\n Columns as fields solution\")\n",
    "field_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex3. Rewrite query from ex1 to dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the complex queries in relational databases require joins. Spark SQL have them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# joins using plain SQL queries\")\n",
    "spark.sql('select * from sales join item_prices on sales.item_id = item_prices.item_id').show()\n",
    "\n",
    "print(\"# using Dataframe API - duplicated item_id column!\")\n",
    "sales_df.join(item_prices_df, sales_df.item_id == item_prices_df.item_id, \"inner\").show()\n",
    "\n",
    "print(\"# dropping redundant column\")\n",
    "sales_with_unit_prices_df = sales_df\\\n",
    "    .join(item_prices_df, sales_df.item_id == item_prices_df.item_id)\\\n",
    "    .drop(sales_df.item_id)\n",
    "    \n",
    "sales_with_unit_prices_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex4. Filter out excluded items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a Spark Dataframe with a column of items select all transactions from sales_df not containing any of these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Dataframe with column of items we would like to exclude\")\n",
    "excluded_items_df = spark.createDataFrame([(\"ITEM_2\",),(\"ITEM_4\",)], ['item'])\n",
    "excluded_items_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to add a column in dataframe based on values from other columns. __withColumn()__ method is just for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_df = sales_with_unit_prices_df\\\n",
    "    .withColumn(\"total_sales\", f.col(\"qty\") * f.col(\"unit_price\"))\n",
    "\n",
    "print(\"# Added new total_sales column which is a multuply of unit_price and qty\")\n",
    "total_sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from simple ooperations we may use complex predicates while calculating the new value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Adding price category column based on a condition\")\n",
    "sales_with_transaction_category = total_sales_df\\\n",
    "    .withColumn(\"transaction_price_category\", \\\n",
    "                f.when(f.col(\"total_sales\") > 150, \"High\")\\\n",
    "                .when(f.col(\"total_sales\") < 60, \"Low\")\\\n",
    "                .otherwise(\"Medium\"))\n",
    "\n",
    "sales_with_transaction_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex5. Two-packs of items\n",
    "We want to create two-packs of items, but sum of their prices must be lower than 360.\n",
    "hint: use cross join, and alias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw a simple aggregation when calculating mean of prices. Dataframe API allows us to make do it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# using alias to have a better column name\")\n",
    "total_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.sum(total_sales_df.total_sales).alias(\"sales\"))\\\n",
    "    .orderBy(f.col(\"sales\").desc())\\\n",
    "    .show()\n",
    "    # .orderBy(total_sales_df.sales) won't work as total_sales_df has no sales column (we define it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex6. Aggregating data to lists\n",
    "Produce a column with list of all shops where each item was sold, new column should be named \"shops\"\n",
    "hint: check collect_list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Date handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of datasets contain some kind of notion of date or time. Let's see how can we transform it. Our total_sales_df contains *transaction_date* column, we are going to extract each bit out of it with functions from pyspark.sql.functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# extracting multiple elements of date\")\n",
    "total_sales_df\\\n",
    "    .withColumn(\"year\", f.year(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"month\", f.month(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day\", f.dayofmonth(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"transaction_date\")))\\\n",
    "    .withColumn(\"day_of_week\", f.date_format(f.col(\"transaction_date\"), 'u'))\\\n",
    "    .withColumn(\"day_of_week_str\", f.date_format(f.col(\"transaction_date\"), 'E'))\\\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to define new column to use obtained values. Here's an example of getting sales aggregated by week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# aggregate sales by week\")\n",
    "total_sales_df\\\n",
    "    .groupBy(f.weekofyear(f.col(\"transaction_date\")))\\\n",
    "    .agg(f.sum(f.col(\"total_sales\")))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex7. Weekly sales aggregation not starting on Monday\n",
    "For Spark, each week starts on Monday. But what if we want to start aggregation on a different day, for example Sunday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex8. (Homework) Sales aggregation with preserving date\n",
    "Sometimes we want to preserve the date of the week (for example last day) instead of year week number. Try implementing aggregation above where instead of week number there is a date of last day of given week. Try to do it without using join. \n",
    "\n",
    "hint: maybe *next_day()* function will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Using results of one query in another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get results from one query to another? We could use joins, but there are other ways. Let's say we want to add maximal price of all items to each sales row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Calculate global max unit_price\")\n",
    "item_prices_df\\\n",
    "    .select(f.max(item_prices_df.unit_price).alias(\"max_price\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how can we get that value out of the dataframe? If we only have one Row then using *first()* method will be enpough to return a Row object. In cases we have more rows then we need to use *collect()*.\n",
    "Both of these methods are actions, which means they will invoke calculations. Furthermore, result of these operations will be sent directly to driver. This can be be problematic for large Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date_row = item_prices_df\\\n",
    "    .select(f.max(item_prices_df.unit_price).alias(\"max_price\"))\\\n",
    "    .first() # first() returns first Row, collect returns list of rows\n",
    "    #.collect()[0]\n",
    "\n",
    "print(max_date_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with a Row object. It has a field for each column in the Dataframe. We can extract values either by index or by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_date_row[0])\n",
    "print(max_date_row.max_price)\n",
    "print(max_date_row['max_price'])\n",
    "\n",
    "max_price = max_date_row.max_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to include it in each sales rows, we need to add a new column of literal value using __lit()__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# adding it as a literal (constant) column\")\n",
    "sales_with_max_global_price_df = total_sales_df\\\n",
    "    .withColumn(\"global_max_price\", f.lit(max_price))\n",
    "\n",
    "sales_with_max_global_price_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex8. Adding constant column using cross join\n",
    "Most of the times we want to avoid unnnecessary actions. They break the flow of a query, canot be optimized and require sending data over network to driver and, in our case, back again. Let's implement query above using cross join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must make sure that Dataframe used in cross join has at most couple elements, if not then the number of rows will explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform partial aggregations but preserving initial number of rows we could use joins.Let's try that to get latest transaction date for each shop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date_by_store_df = total_sales_df\\\n",
    "    .groupBy(f.col(\"shop_id\"))\\\n",
    "    .agg(f.max(\"transaction_date\").alias(\"max_transaction_date_by_shop\")) \n",
    "    \n",
    "total_sales_df.join(max_date_by_store_df, [\"shop_id\"])\\\n",
    "    .show() # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Side note__: instead of join condition we passed a list with one column name. This way we avoid duplicationd of that column in join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is using __window functions__. They are experimental since spark 1.4, but they are widely used in production.\n",
    "\n",
    "First, we need to define Window with a grouping column using __partitionBy()__ method. Then we create a new column as usual, but after invoking a function we add __.over(window)__ to say that this is not a global operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(f.col(\"shop_id\"))\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"max_transaction_date_by_shop\", f.max(f.col(\"transaction_date\")).over(window)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each partition partition we can specify order in which we traverse the rows. Let's use that to find ordinals for transactions for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_by_item_sorted = Window.partitionBy(f.col(\"item_id\")).orderBy(f.col(\"transaction_date\"))\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"item_transaction_ordinal\", f.rank().over(window_by_item_sorted))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify also how large the window should be in each step using **rowsBetween()** method. We can use it to find average price from last two transactions in given shop, ordered by transaction date (like a group-level, moving average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_by_transaction_date = Window\\\n",
    "    .partitionBy(f.col(\"shop_id\"))\\\n",
    "    .orderBy(f.col(\"transaction_date\"))\\\n",
    "    .rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "total_sales_df\\\n",
    "    .withColumn(\"price_moving_average\", f.mean(f.col(\"total_sales\")).over(window_by_transaction_date))\\\n",
    "    .orderBy(f.col(\"shop_id\"), f.col(\"transaction_date\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex9. Cumulative moving average of quantities\n",
    "Find average quantity of items from current and all previous transactions for given item ordered by transaction date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Complex aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need more complex aggregations. Let's say we want to analyse weekly sales in each shop. We would like to get a Dataframe with one row per shop and a list of all transactions with week and year numbers.\n",
    "We could try doing that using multiple invocations of collect_list function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_by_shop_df = total_sales_df\\\n",
    "    .groupBy(\"shop_id\", f.weekofyear(\"transaction_date\").alias(\"week\"), f.year(\"transaction_date\").alias(\"year\"))\\\n",
    "    .agg(f.sum(\"total_sales\").alias(\"sales\"))\n",
    "\n",
    "print(\"# Sales Dataframe with week and year columnns\")\n",
    "weekly_sales_by_shop_df.show()\n",
    "        \n",
    "print(\"# aggregating sales with three collect_list invocations\")\n",
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(\"week\"),f.collect_list(\"year\"),  f.collect_list(\"sales\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately solution above won't work correctly, as ordering in each column may be different. Passing list of columns to colllect_list won't work either as there is no such API: .agg(f.collect_list([\"sales\", \"week\"]))  \n",
    "\n",
    "We can overcome that using a struct method which aggregates values for each row in a structure (similar to dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sales_weekly_series_df = weekly_sales_by_shop_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "\n",
    "shop_sales_weekly_series_df.show(truncate=False)\n",
    "shop_sales_weekly_series_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a time series for each shop, but what if we want to have it ordered by date? We could try sorting the dataframe before aggregation. Unfortunately Spark doesn't preserve this ordering after groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_weekly_sales_df = weekly_sales_by_shop_df\\\n",
    "    .orderBy(\"shop_id\", \"year\", \"week\")\n",
    "  \n",
    "ordered_weekly_sales_df.show()\n",
    "\n",
    "wrongly_sorted_series_df = ordered_weekly_sales_df\\\n",
    "    .groupBy(\"shop_id\")\\\n",
    "    .agg(f.collect_list(f.struct([\"year\", \"week\", \"sales\"])).alias(\"sales_ts\"))\n",
    "    \n",
    "wrongly_sorted_series_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Defining custom UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve that issue we need to sort the time series after aggregation. To do that we need to define a custom User Defined Functions (UDF).\n",
    "Such function is invoked on each row separately. It can take as many columns as we need. It can contain any custom Python code, even from libraries available in your environment. \n",
    "\n",
    "Let's create a function appending a custom prefix to value from another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_function(column1):\n",
    "    return \"AFTER_UDF_\" + str(column1)\n",
    "\n",
    "my_custom_udf = f.udf(my_custom_function)\n",
    "df_after_udf = shop_sales_weekly_series_df.withColumn(\"sales_ts_after_udf\", my_custom_udf(f.col(\"sales_ts\")))\n",
    "df_after_udf.show()\n",
    "print(\"# Schema of the new dataframe\")\n",
    "df_after_udf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use custom UDF in a plain, SQL query we need to register it in a sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext.registerFunction(\"my_udf\", my_custom_function)\n",
    "\n",
    "spark.sql(\"select my_udf(shop_id) from sales\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex10. UDF calculating sales for given transaction by multiplying qty and unit_price  \n",
    "Create a UDF taking two columns (qty and unit_price) from total_sales_df and returning their product as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we want to return more than one column from a UDF? Let's try returning a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_shop_id(shop_id):\n",
    "    s, i = shop_id.split(\"_\")\n",
    "    return s, int(i) \n",
    "\n",
    "split_shop_id_udf = f.udf(split_shop_id)\n",
    "df_udf_no_schema = shop_sales_weekly_series_df.withColumn(\"shop_id_splits\", split_shop_id_udf(f.col(\"shop_id\")))\n",
    "print(\"# Results not as expected - seems like calling toString on Object\")\n",
    "df_udf_no_schema.show(truncate=False)\n",
    "\n",
    "print(\"# Actual inferred schema: one string instead of a tuple\")\n",
    "df_udf_no_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid that situation we need to define a result schema for our UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "\n",
    "schema = StructType([StructField(\"s\", StringType()), StructField(\"i\", IntegerType())])\n",
    "udf_with_schema = f.udf(split_shop_id, schema)\n",
    "\n",
    "df = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", udf_with_schema(f.col(\"shop_id\")))\n",
    "df.show(truncate=False)\n",
    "print(\"# New schema is correct as well\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating multiple columns based on a result from UDF\n",
    "In the last example we created one column with multiple values, now let's try to extract them to separate columns\n",
    "\n",
    "This can be done using asterisk __\\*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_shop_id = df.select(\"*\", \"shop_id_splits_with_schema.*\").drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id.show()\n",
    "print(\"# Schema was updated and new fields have correct types\")\n",
    "df_split_shop_id.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution above will invoke UDF as many times as new column (it's a feature not a bug! https://issues.apache.org/jira/browse/SPARK-17728\").\n",
    "\n",
    "For costly UDFs (and in pySpark most of them are very costly) we have a workaround: we need to explode an array with one element - result of the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_shop_id_correct = df_udf_no_schema.withColumn(\"shop_id_splits_with_schema\", \\\n",
    "                                 f.explode(f.array(udf_with_schema(f.col(\"shop_id\")))))\n",
    "\n",
    "df_split_shop_id_correct = df_split_shop_id_correct \\\n",
    "    .select(\"*\", \"shop_id_splits_with_schema.*\") \\\n",
    "    .drop(\"shop_id_splits_with_schema\")\n",
    "df_split_shop_id_correct.show()\n",
    "print(\"# Results and schema are the same\")\n",
    "df_split_shop_id_correct.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we know that this UDF will be invoked multiple times? Let's take a deeper look at execution plans of both queries.\n",
    "\n",
    "For the first version we can see:\n",
    "\n",
    "> +- BatchEvalPython [split_shop_id(shop_id#10), split_shop_id(shop_id#10), split_shop_id(shop_id#10)], [shop_id#10, sales_ts#3899, pythonUDF0#4442, pythonUDF1#4443, pythonUDF2#4444]\n",
    "\n",
    "which contains multiple pythonUDF references.\n",
    "                                  \n",
    "For the updated solution there's only one invocation:\n",
    "\n",
    "> +- BatchEvalPython [split_shop_id(shop_id#10)], [shop_id#10, sales_ts#3899, shop_id_splits#4155, pythonUDF0#4448]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_shop_id.explain()\n",
    "print(\"\\n\\n\\n\")\n",
    "df_split_shop_id_correct.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex.11 Sort each time series in wrongly_sorted_series_df from previous exercise in descending order and compare to initial ts \n",
    "tip: use python's sorted method inside a UDF. FloatType and ArrayType imports may be usefull as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
