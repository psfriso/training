{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env_setup\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "\n",
    "spark = env_setup.getSession(local=True)\n",
    "sales_df = spark.table(\"sales\")\n",
    "item_prices_df = spark.table(\"item_prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas UDFs\n",
    "\n",
    "Many data science libraries use pandas dataframes for operations. We would like to use them in our spark UDF easily. Before Spark 2.3 we had to manually map our column to pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, FloatType, StructType, StructField, DateType, DoubleType\n",
    "\n",
    "@udf(StringType())\n",
    "def old_udf_type(col):\n",
    "    return str(type(col))\n",
    "\n",
    "@udf(StringType())\n",
    "def old_udf(col):\n",
    "    return \">>>\" + str(col) + \"<<<\" # + is invoked on string\n",
    "\n",
    "@udf(StringType())\n",
    "def old_udf_pandas(col):\n",
    "    pds = pd.Series(col) # creating pandas Series\n",
    "    return str(len(pds)) # using pandas api, need to return python list\n",
    "#     return str(pds.sum()) \n",
    "\n",
    "df1 = spark.createDataFrame([(1,),(2,), (3,)], ['data'])\n",
    "df2 = spark.createDataFrame([([1,2,3],),([2],)], ['data'])\n",
    "\n",
    "df1.select(\"data\", old_udf_type(\"data\"), old_udf(\"data\"), old_udf_pandas(\"data\")).show(truncate=False)\n",
    "df2.select(\"data\", old_udf_type(\"data\"), old_udf(\"data\"), old_udf_pandas(\"data\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar pandas udf\n",
    "\n",
    "The type of variable passed to the udf is exactly the same as the one from schema. If we create a pandas Series from the input, then in we will have __one Series per row__. Its size will differ based on the actual value (and its type).\n",
    "\n",
    "Spark 2.3 introduced pandas_udf function, which treats the input to udf as a pandas Series __but in a different way__. In pandas_udf there will be at most as many series as there are rows. Spark can optimize it and put values from multiple rows inside one Series.\n",
    "\n",
    "To use pandas udfs we need to install pyarrow package by invoking in our terminal:\n",
    "```conda install pyarrow```\n",
    "Then a kernel restart should be needed to have it available.\n",
    "\n",
    "To have behavior similar to normal UDFs (where we have one to one mapping between rows) we need to use __SCALAR__ pandas udf. We specify that as a second argument to the pandas_udf function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "def pudf_type(col_as_pd_series):\n",
    "    return pd.Series([str(type(col_as_pd_series))]) # It may not work on bigger dataset because we always return 1 element!\n",
    "\n",
    "@pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "def pudf_str_series(col_as_pd_series):\n",
    "    return pd.Series([str(col_as_pd_series)]) # as above, may not work on bigger dataset\n",
    "\n",
    "@pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "def pudf_length(col_as_pd_series):\n",
    "    # + below is invoked on pandas.Series\n",
    "    return \"##\" + col_as_pd_series.astype(str) + \"##\" + str(len(col_as_pd_series)) \n",
    "\n",
    "\n",
    "\n",
    "df1.select(\"data\", pudf_type(\"data\"), pudf_str_series(\"data\"), pudf_length(\"data\")).show(truncate=False)\n",
    "df2.select(\"data\", pudf_type(\"data\"), pudf_str_series(\"data\"), pudf_length(\"data\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCALAR pandas udf is a mapping between two pandas Series, where number of elements in the output is the same as the input. pandas udf can change the type of a column on which it's invoked.\n",
    "\n",
    "SCALAR pandas udfs are useful when we already have a code working on pandas series. Previously Data Scientists tried calling toPandas() on Spark dataframe and then performing calculations on the driver. This has two disadvantages:\n",
    "1. It won't work for datasets that do not fit into driver's memory\n",
    "2. Can't be parallelized (only via pandas parallelization on driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped_agg pandas udf\n",
    "The second type of pandas_udf is __GROUPED_AGG__ thanks to which we can define our user defined aggregate functions (UDAF). It can be seen as a mapping between pandas Series and a scalar invoked for each aggregation group. \n",
    "In previous versions of Spark, to achieve that we had to collect elements to list, map to pandas and then return a scalar, now it's much simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(FloatType(), PandasUDFType.GROUPED_AGG)\n",
    "def pudf_mean(group):\n",
    "    print(group)\n",
    "    return group.astype(int).mean() # our own implementation of mean() udf using pandas\n",
    "\n",
    "sales_df.show()\n",
    "sales_df.groupby(\"shop_id\").agg(pudf_mean(\"qty\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it also as window functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window \n",
    "\n",
    "w = Window.partitionBy(\"shop_id\")\n",
    "new_df = sales_df.select(\"shop_id\", pudf_mean(\"qty\").over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows will new_df have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.count()\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course as many as there are rows in the input DF, this is what windows are for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped_map pandas udf\n",
    "\n",
    "There is another possibility of defining our pandas udf called __GROUPED_MAP__. It can be seen as a mapping from pd.DataFrame to another pd.DataFrame. \n",
    "\n",
    "The return type mut be a StructType corresponding to the types of returned pd.DataFrame. \n",
    "Number of rows can be arbitrary. \n",
    "\n",
    "It must be invoked on GroupedData (after groupby or inside a window). The input pd.DataFrame contains all columns from the input spark.Dataframe (aggregating column is also present) but with limited number of rows to each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"shop_id string, mean double\", PandasUDFType.GROUPED_MAP)\n",
    "def gm_pudf(pdf):\n",
    "    pdf['mean'] = pdf['qty'].astype(int).mean()\n",
    "    return pdf[['shop_id','mean']]\n",
    "\n",
    "sales_df.groupby(\"shop_id\").apply(gm_pudf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a possibility to get the aggregating column as a variable inside udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"mapped_shop_id string, mean double\", PandasUDFType.GROUPED_MAP)\n",
    "def gm_pudf(key, pdf):\n",
    "    pdf['mean'] = pdf['qty'].astype(int).mean()\n",
    "    pdf['mapped_shop_id'] = pdf['shop_id'] + \"#\" + key\n",
    "    return pdf[['mapped_shop_id','mean']]\n",
    "\n",
    "sales_df.groupby(\"shop_id\").apply(gm_pudf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__additional notes:__ \n",
    "1. Udfs are considered deterministic (and may be invoked multiple times), if you have a nondeterministic function then use .asNondeterministic() method.\n",
    "2. Each group must fit into memory of the worker node. Skewed dataset may result in OOM.\n",
    "3. Only unbounded windows are supported for pandas UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 1: Find correlation between temperature and humidity for each month using pandas UDFs\n",
    "\n",
    "\n",
    "In data folder there are two csv files from Kaggle https://www.kaggle.com/codersree/mount-rainier-weather-and-climbing-data\n",
    "\n",
    "Read Rainer_Weather.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_schema = StructType([\n",
    "    StructField(\"Date\", StringType()),\n",
    "    StructField(\"Battery Voltage AVG\", DoubleType()),\n",
    "    StructField(\"Temperature AVG\", DoubleType()),\n",
    "    StructField(\"Relative Humidity AVG\", DoubleType()),\n",
    "    StructField(\"Wind Speed Daily AVG\", DoubleType()),\n",
    "    StructField(\"Wind Direction AVG\", DoubleType()),\n",
    "    StructField(\"Solare Radiation AVG\", DoubleType())\n",
    "])\n",
    "rainer_weather_df = spark.read.option(\"header\", \"true\").csv(path=\"../data/Rainier_Weather.csv\", schema=weather_schema)\n",
    "rainer_weather_df.show(2)\n",
    "print(rainer_weather_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Map \"Date\" field to Date Type (to_date function)\n",
    "2. Define pandas UDF to calculate correlation for of Temperature AVG and  Relative Humidity AVG for each month (try using grouped_agg UDF)\n",
    "3. Invoke udf and show the result ordered by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same using GROUPED_MAP UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Side note__ Spark's dataframes have some statistical functions available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month.where(f.col(\"month\") == \"12\").stat.corr(\"Temperature AVG\", \"Relative Humidity AVG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML\n",
    "\n",
    "Spark ML operates on Dataframes containing columns of Vectors. Let's see how we can use vector assembler to transform our data\n",
    "\n",
    "Let's try to predict temperature based on other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "label_col = 'Temperature AVG'\n",
    "cols = df_month.columns\n",
    "print(cols)\n",
    "cols.remove('Date')\n",
    "cols.remove(label_col)\n",
    "print(cols)\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "transformed_df = vecAssembler.transform(df_month).select(label_col, \"features\")\n",
    "transformed_df.show(2, False)\n",
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split data into train and test datasets (why is that needed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = transformed_df.randomSplit([.8,.2], seed=111)\n",
    "print(transformed_df.count())\n",
    "print(\"Train dataset size:\")\n",
    "print(train_data.count())\n",
    "print(\"Test dataset size:\")\n",
    "print(test_data.count())\n",
    "print(\"Number of features:\")\n",
    "print(str(len(cols)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting linear regression model and printing basic information. \n",
    "Intercept is f(0). coefficients are the coefficients for each of the feature.  r2 is the coefficient of determination, it explains how much of the response variable variation is explained by the model (defined as r2 = 1-SS_res/SS_tot) - closer to 1 is usually better. Unfortunately there's no p-value for significance of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=label_col)\n",
    "\n",
    "linear_model = lr.fit(train_data)\n",
    "print(\"intercept: \")\n",
    "print(linear_model.intercept)\n",
    "print(\"Coefficients:\")\n",
    "print(linear_model.coefficients)\n",
    "print(\"RMSE:\")\n",
    "print(linear_model.summary.rootMeanSquaredError)\n",
    "print(\"R^2:\")\n",
    "print(linear_model.summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have test_data set with labels then we can evaluate it. linear model will automatically get features from the \"features\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE:\")\n",
    "print(linear_model.evaluate(test_data).rootMeanSquaredError)\n",
    "print(\"R^2: \")\n",
    "print(linear_model.evaluate(test_data).r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't have labels we can still use transform method to get results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_model.transform(test_data)\n",
    "predictions.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is not performing very well, but let's save it, load it and use for predictions. The __class__ we're using for loading the model __is different__ than the one we used for fitting model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "import shutil\n",
    "shutil.rmtree(\"linear_model\", ignore_errors=True)\n",
    "linear_model.save(\"linear_model\")\n",
    "loaded_lm = LinearRegressionModel.load(\"linear_model\")\n",
    "loaded_lm.transform(test_data).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipelines\n",
    "\n",
    "Most of ML systems work exactly the same:\n",
    "1. preprocess features (transforming dataframes)\n",
    "2. train model - sometimes with cross validation (generating object which will transform dataframes)\n",
    "3. use model for prediction (transforming dataframes)\n",
    "\n",
    "In Spark ML, all these three steps are defined using two type: `Transformer` and `Estimator`. The first one transforms dataframes and the second one fits a Transformer model.\n",
    "\n",
    "We can define a `Pipeline` using only those types. We already have them defined, we just need to combine them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vectorAssemblerTransformer = vecAssembler\n",
    "modelEstimator = lr\n",
    "pipeline = Pipeline(stages = [vectorAssemblerTransformer, modelEstimator])\n",
    "\n",
    "model = pipeline.fit(df_month)\n",
    "model.transform(df_month).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example above didn't include splitting data into train and test datasets. Let's do something better and use TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(modelEstimator.maxIter, [5, 10]).build()\n",
    "regression_evaluator = RegressionEvaluator().setLabelCol(label_col).setMetricName(\"rmse\")\n",
    "tvs = TrainValidationSplit(estimator=modelEstimator, \n",
    "                           estimatorParamMaps=grid, \n",
    "                           evaluator=regression_evaluator, \n",
    "                           trainRatio=0.8)\n",
    "\n",
    "new_pipeline = Pipeline(stages=[vectorAssemblerTransformer, tvs])\n",
    "model = new_pipeline.fit(df_month)\n",
    "model.transform(df_month).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking summary of a pipeline model is trickier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stages[-1].bestModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it with the RegressionEvaluator metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.stages[-1].getEvaluator().getMetricName())\n",
    "print(list(zip(model.stages[-1].validationMetrics, model.stages[-1].getEstimatorParamMaps())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex.2 Check if a different regression model (DecisionTreeRegressor) will have a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(zip(cols,list(best_model.featureImportances))),key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "dt_cv = CrossValidator(estimator=dt, \n",
    "                           estimatorParamMaps=dt_grid, \n",
    "                           evaluator=RegressionEvaluator().setLabelCol(label_col).setMetricName(\"rmse\"), \n",
    "                           numFolds=4)\n",
    "\n",
    "dt_cv_pipeline = Pipeline(stages=[vectorAssemblerTransformer, dt_cv])\n",
    "dt_cv_model = dt_cv_pipeline.fit(df_month)\n",
    "best_cv_model = dt_model.stages[-1].bestModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_cv_model.stages[-1].getEvaluator().getMetricName())\n",
    "print(list(zip(dt_cv_model.stages[-1].avgMetrics, dt_cv_model.stages[-1].getEstimatorParamMaps())))\n",
    "print(sorted(list(zip(cols,list(best_cv_model.featureImportances))),key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using cross validation the rmse is a bit higher - hopefully it is not overfitting as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 3. Train the best model to predict probability of successfuly reaching summit of Mt. Rainer.\n",
    "Use `climbing_statistics.csv` file. It may be tricky to join these two datasets. For categorical variable (`Route`) use some encoding (for example OneHotEncoder). Think about what kind of model do you need - is regression the best option here? What problems can it cause? Hint: Check LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
